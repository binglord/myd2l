{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 线性代数\n",
    "将介绍线性代数中的基本数学对象、算术和运算，并⽤数学符号和相应的代码实现来表⽰它们。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 标量\n",
    "如果你曾经在餐厅⽀付餐费，那么应该已经知道⼀些基本的线性代数，⽐如在数字间相加或相乘。例如，北京的温度为52◦F（华⽒度，除摄⽒度外的另⼀种温度计量单位）。\\\n",
    "如果要将此华⽒度值转换为更常⽤的摄⽒度，则可以计算表达式: c = (5/9) * (f -32),并将f赋为52。在此等式中，每⼀项（5、 9和32）都是标量值。符号c和f称为变量（variable），它们表⽰未知的标量值。\\\n",
    "\n",
    "本书采⽤了数学表⽰法，其中标量变量由普通⼩写字⺟表⽰（例如， x、 y和z）。本书⽤R（重叠R）表⽰所有（连续）实数标量的空间，之后将严格定义空间（space）是什么，但现在只要记住表达式x 属于 R是表⽰x是⼀个实值标量的正式形式。符号 属于 称为“属于”，它表⽰“是集合中的成员”。例如x， y 属于 {0,1}，可以⽤来表明x和y是值只能为0或1的数字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(3.0)\n",
    "y = torch.tensor(2.0)\n",
    "x + y, x * y, x / y, x ** y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 向量\n",
    "向量可以被视为标量值组成的列表。\\\n",
    "当向量表⽰数据集中的样本时，它们的值具有⼀定的现实意义。例如，如果我们正在训练⼀个模型来预测贷款违\n",
    "约⻛险，可能会将每个申请⼈与⼀个向量相关联，其分量与其收⼊、⼯作年限、过往违约次数和其他因素相\n",
    "对应。如果我们正在研究医院患者可能⾯临的⼼脏病发作⻛险，可能会⽤⼀个向量来表⽰每个患者，其分量\n",
    "为最近的⽣命体征、胆固醇⽔平、每天运动时间等。\\\n",
    "在数学表⽰法中，向量通常记为**粗体、⼩写**的符号（例如， x、 y和z)）。\\\n",
    "⼈们通过⼀维张量表⽰向量。⼀般来说，张量可以具有任意⻓度，取决于机器的内存限制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4) # ⼀维张量表⽰向量\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以使⽤下标来引⽤向量的任⼀元素，例如可以通过xi来引⽤第i个元素。\\\n",
    "注意，元素xi是⼀个标量，所以我们在引⽤它时不会加粗。\\\n",
    "⼤量⽂献认为列向量是向量的默认⽅向，在本书中也是如此。\\\n",
    "其中x1; : : : ; xn是向量的元素。在代码中，我们通过张量的索引来访问任⼀元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**⻓度、维度和形状** \\\n",
    "向量只是⼀个数字数组，就像每个数组都有⼀个⻓度⼀样，每个向量也是如此。\\\n",
    "在数学表⽰法中，如果我们想说⼀个向量x由n个实值标量组成，可以将其表⽰为**x** 属于 R^n。向量的⻓度通常称为向量的维度（dimension）。与普通的Python数组⼀样，我们可以通过调⽤Python的内置len()函数来访问张量的⻓度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x) # 长度、维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**形状** \\\n",
    "当⽤张量表⽰⼀个向量（只有⼀个轴）时，我们也可以通过.shape属性访问向量的⻓度。形状（shape）是⼀个元素组，列出了张量沿每个轴的⻓度（维数）。对于只有⼀个轴的张量，形状只有⼀个元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请注意，维度（dimension）这个词在不同上下⽂时往往会有不同的含义，这经常会使⼈感到困惑。为了清楚起⻅，我们在此明确⼀下：**向量或轴的维度被⽤来表⽰向量或轴的⻓度，即向量或轴的元素数量。**然⽽，**张量的维度⽤来表⽰张量具有的轴数。**在这个意义上，张量的某个轴的维数就是这个轴的⻓度。\n",
    "### 2.3.3 矩阵\n",
    "正如向量将标量从零阶推⼴到⼀阶，矩阵将向量从⼀阶推⼴到⼆阶。矩阵，我们通常⽤**粗体、⼤写**字⺟来表⽰（例如，** X、 Y和Z**），在代码中表⽰为具有两个轴的张量。\\\n",
    "当调⽤函数来实例化张量时，我们可以通过指定两个分量m和n来创建⼀个形状为m × n的矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15],\n",
      "        [16, 17, 18, 19]])\n",
      "tensor([12, 13, 14, 15])\n",
      "tensor(12)\n"
     ]
    }
   ],
   "source": [
    "A = torch.arange(20).reshape(5,4)\n",
    "print(A)\n",
    "print(A[3])\n",
    "print(A[3,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在在代码中访问**矩阵的转置。**\n",
    "转置：列变成行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  4,  8, 12, 16],\n",
       "        [ 1,  5,  9, 13, 17],\n",
       "        [ 2,  6, 10, 14, 18],\n",
       "        [ 3,  7, 11, 15, 19]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作为⽅阵的⼀种特殊类型，对称矩阵（symmetric matrix） A等于其转置：这⾥定义⼀个对称矩阵B："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [2, 0, 4],\n",
      "        [3, 4, 5]])\n",
      "tensor([[1, 2, 3],\n",
      "        [2, 0, 4],\n",
      "        [3, 4, 5]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.tensor([[1,2,3],[2,0,4],[3,4,5]])\n",
    "print(B)\n",
    "print(B.T)\n",
    "B == B.T # 现在我们将B与它的转置进⾏⽐较。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矩阵是有⽤的数据结构：它们允许我们组织具有不同模式的数据。\\\n",
    "例如，我们矩阵中的⾏可能对应于不同的房屋（数据样本），⽽列可能对应于不同的属性。\\\n",
    "因此，尽管单个向量的默认⽅向是列向量，但在表⽰表格数据集的矩阵中，**将每个数据样本作为矩阵中的⾏向量更为常⻅。**后⾯的章节将讲到这点，这种约定将⽀持常⻅的深度学习实践。例如，沿着张量的最外轴，我们可以访问或遍历⼩批量的数据样本。\n",
    "### 2.3.4 张量\n",
    "张量是描述具有任意数量轴的n维数组的通⽤⽅法。\\\n",
    "例如，向量是⼀阶张量，矩阵是⼆阶张量。张量⽤特殊字体的⼤写字⺟表⽰（例如， X、 Y和Z），它们的索引机制（例如xijk和[X]1;2i−1;3）与矩阵类似。\\\n",
    "\n",
    "当我们开始处理图像时，张量将变得更加重要，图像以n维数组形式出现，其中3个轴对应于⾼度、宽度，以及⼀个通道（channel）轴，⽤于表⽰颜⾊通道（红⾊、绿⾊和蓝⾊）。现在先将⾼阶张量暂放⼀边，⽽是专注学习其基础知识。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(24).reshape(2,3,4)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.5 张量算法的基本性质\n",
    "标量、向量、矩阵和任意数量轴的张量（本⼩节中的“张量”指代数对象）有⼀些实⽤的属性。\\\n",
    "例如，从按元素操作的定义中可以注意到，任何按元素的⼀元运算都不会改变其操作数的形状。\\\n",
    "同样，给定具有相同形状的任意两个张量，任何按元素⼆元运算的结果都将是相同形状的张量。例如，将两个相同形状的矩阵相加，会在这两个矩阵上执⾏元素加法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([[ 0.,  2.,  4.,  6.],\n",
       "         [ 8., 10., 12., 14.],\n",
       "         [16., 18., 20., 22.],\n",
       "         [24., 26., 28., 30.],\n",
       "         [32., 34., 36., 38.]]))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20, dtype=torch.float32).reshape(5,4)\n",
    "B = A.clone() # 通过分配新内存，将A的⼀个副本分配给B\n",
    "A, A + B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**按元素乘法** \\\n",
    "两个矩阵的按元素乘法称为Hadamard积（Hadamard product）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([[  0.,   1.,   4.,   9.],\n",
       "         [ 16.,  25.,  36.,  49.],\n",
       "         [ 64.,  81., 100., 121.],\n",
       "         [144., 169., 196., 225.],\n",
       "         [256., 289., 324., 361.]]))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A, B, A * B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将张量乘以或加上⼀个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2,  3,  4,  5],\n",
       "          [ 6,  7,  8,  9],\n",
       "          [10, 11, 12, 13]],\n",
       " \n",
       "         [[14, 15, 16, 17],\n",
       "          [18, 19, 20, 21],\n",
       "          [22, 23, 24, 25]]]),\n",
       " torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 2\n",
    "X = torch.arange(24).reshape(2,3,4)\n",
    "a + X, (a + X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.6 降维\n",
    "我们可以对任意张量进⾏的⼀个有⽤的操作是计算其元素的和。\\\n",
    "在代码中可以调⽤计算求和的函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.]), tensor(6.))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4, dtype=torch.float32)\n",
    "x, x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以表⽰任意形状张量的元素和。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " torch.Size([5, 4]),\n",
       " tensor(190.))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A, A.shape, A.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认情况下，调⽤求和函数会沿所有的轴降低张量的维度，使它变为⼀个标量。\\\n",
    "我们还可以指定张量沿哪⼀个轴来通过求和降低维度。\\\n",
    "以矩阵为例，为了通过求和所有⾏的元素来降维（轴0），可以在调⽤函数时指定axis=0。由于输⼊矩阵沿0轴降维以⽣成输出向量，因此输⼊轴0的维数在输出形状中消失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([40., 45., 50., 55.]),\n",
       " torch.Size([4]),\n",
       " tensor([ 6., 22., 38., 54., 70.]),\n",
       " torch.Size([5]))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum_axis0 =  A.sum(axis=0)\n",
    "A_sum_axis1 = A.sum(axis=1)\n",
    "A, A_sum_axis0, A_sum_axis0.shape, A_sum_axis1, A_sum_axis1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "沿着⾏和列对矩阵求和，等价于对矩阵的所有元素进⾏求和。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(190.), tensor(190.))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(), A.sum(axis=[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⼀个与求和相关的量是**平均值（mean或average）。我们通过将总和除以元素总数来计算平均值。**在代码中，我们可以调⽤函数来计算任意形状张量的平均值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor(9.5000),\n",
       " 9.5,\n",
       " tensor(9.5000))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A, A.mean(), 190/20, A.sum() / A.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样，计算平均值的函数也可以沿指定轴降低张量的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([ 8.,  9., 10., 11.]),\n",
       " tensor([ 8.,  9., 10., 11.]))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A, A.mean(axis=0), A.sum(axis=0)/ A.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**非降维求和** \\\n",
    "有时在调⽤函数来计算总和或均值时保持轴数不变会很有⽤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 6.],\n",
       "         [22.],\n",
       "         [38.],\n",
       "         [54.],\n",
       "         [70.]]),\n",
       " torch.Size([5, 1]))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_A = A.sum(axis=1, keepdims=True)\n",
    "sum_A, sum_A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例如，由于sum_A在对每⾏进⾏求和后仍保持两个轴，我们可以通过⼴播将A除以sum_A。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([[ 6.],\n",
       "         [22.],\n",
       "         [38.],\n",
       "         [54.],\n",
       "         [70.]]),\n",
       " tensor([[0.0000, 0.1667, 0.3333, 0.5000],\n",
       "         [0.1818, 0.2273, 0.2727, 0.3182],\n",
       "         [0.2105, 0.2368, 0.2632, 0.2895],\n",
       "         [0.2222, 0.2407, 0.2593, 0.2778],\n",
       "         [0.2286, 0.2429, 0.2571, 0.2714]]))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A, sum_A, A / sum_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们想沿某个轴计算A元素的累积总和，⽐如axis=0（按⾏计算），可以调⽤cumsum函数。此函数不会沿任何轴降低输⼊张量的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  6.,  8., 10.],\n",
       "         [12., 15., 18., 21.],\n",
       "         [24., 28., 32., 36.],\n",
       "         [40., 45., 50., 55.]]),\n",
       " tensor([[ 0.,  1.,  3.,  6.],\n",
       "         [ 4.,  9., 15., 22.],\n",
       "         [ 8., 17., 27., 38.],\n",
       "         [12., 25., 39., 54.],\n",
       "         [16., 33., 51., 70.]]))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A, A.cumsum(axis=0), A.cumsum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.7 点积（Dot Product）\n",
    "**向量和向量的积** \\\n",
    "我们已经学习了按元素操作、求和及平均值。\\\n",
    "另⼀个最基本的操作之⼀是点积。\\\n",
    "给定两个向量x; y 属于 $$R^d$$ ,它们的点积（dot product） $$X^TY$$ （或⟨x; y⟩）是**相同位置的按元素乘积的和**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.ones(4, dtype=torch.float32)\n",
    "x, y, torch.dot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，我们可以通过执⾏按元素乘法，然后进⾏求和来表⽰两个向量的点积："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.]),\n",
       " tensor([1., 1., 1., 1.]),\n",
       " tensor([0., 1., 2., 3.]),\n",
       " tensor(6.))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y, x * y, torch.sum(x * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "点积在很多场合都很有⽤。\\\n",
    "例如，给定⼀组由向量x 2 Rd表⽰的值，和⼀组由w 2 Rd表⽰的权重。 x中的值根据权重w的加权和，可以表⽰为点积x⊤w。当权重为⾮负数且和为1（即(∑d i=1 wi = 1)）时，点积表⽰加\n",
    "权平均（weighted average）。将两个向量规范化得到单位⻓度后，点积表⽰它们夹⻆的余弦。本节后⾯的内容将正式介绍⻓度（length）的概念。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.8 矩阵-向量积\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**矩阵与向量的积** \\\n",
    "现在我们知道如何计算点积，可以开始理解矩阵-向量积（matrix-vector product）。\\\n",
    "我们可以把⼀个矩阵A 2 Rm×n乘法看作⼀个从Rn到Rm向量的转换。这些转换是⾮常有⽤的，例如可以⽤⽅\n",
    "阵的乘法来表⽰旋转。后续章节将讲到，我们也可以使⽤矩阵-向量积来描述在给定前⼀层的值时，求解神经\n",
    "⽹络每⼀层所需的复杂计算。\n",
    "在代码中使⽤张量表⽰矩阵-向量积，我们使⽤mv函数。当我们为矩阵A和向量x调⽤torch.mv(A, x)时，会执\n",
    "⾏矩阵-向量积。注意， A的列维数（沿轴1的⻓度）必须与x的维数（其⻓度）相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " torch.Size([5, 4]),\n",
       " tensor([0., 1., 2., 3.]),\n",
       " torch.Size([4]),\n",
       " tensor([ 14.,  38.,  62.,  86., 110.]))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A, A.shape, x, x.shape, torch.mv(A,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.9 矩阵-矩阵乘法\n",
    "**矩阵和矩阵的积** \\\n",
    "在掌握点积和矩阵-向量积的知识后，那么矩阵-矩阵乘法（matrix-matrix multiplication）应该很简单。\\\n",
    "假设有两个矩阵 A B \\\n",
    "要⽣成矩阵积C = AB，最简单的⽅法是考虑A的⾏向量和B的列向量: \\\n",
    "当我们简单地将每个元素cij计算为点积a⊤ i bj: \\\n",
    "我们可以将矩阵-矩阵乘法AB看作简单地执⾏m次矩阵-向量积，并将结果拼接在⼀起，形成⼀个n × m矩阵。在下⾯的代码中，我们在A和B上执⾏矩阵乘法。这⾥的A是⼀个5⾏4列的矩阵， B是⼀个4⾏3列的矩阵。两者相乘后，我们得到了⼀个5⾏3列的矩阵。\\\n",
    "A torch.Size([5, 4]), B torch.Size([4, 3]), 列行相等能相乘，得到行列[5,3]大小的结果矩阵 \\\n",
    "乘：A出行，B出列，都是4长度，按位相乘后相加，得出一个元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " torch.Size([5, 4]),\n",
       " tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]),\n",
       " torch.Size([4, 3]),\n",
       " tensor([[ 6.,  6.,  6.],\n",
       "         [22., 22., 22.],\n",
       "         [38., 38., 38.],\n",
       "         [54., 54., 54.],\n",
       "         [70., 70., 70.]]))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.ones(4,3)\n",
    "A, A.shape, B, B.shape, torch.mm(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矩阵-矩阵乘法可以简单地称为矩阵乘法，不应与” Hadamard积”混淆。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.10 范数\n",
    "线性代数中最有⽤的⼀些运算符是范数（norm）。\\\n",
    "⾮正式地说，**向量的范数是表⽰⼀个向量有多⼤**。\\\n",
    "这⾥考虑的⼤⼩（size）概念不涉及维度，⽽是分量的⼤⼩。\\\n",
    "\n",
    "在线性代数中，**向量范数是将向量映射到标量的函数f** \\\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myd2l",
   "language": "python",
   "name": "myd2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
